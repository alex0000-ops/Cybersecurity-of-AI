- --definicia
- Útoky zneužívajúce dôveru v externé komponenty (modely, datasety, knižnice, pluginy) použité v AI systémoch.
  Útok zasahuje komponenty modelu ešte pred jeho nasadením, čím ohrozuje inferenčné prostredie bez priameho prístupu k nemu.
- LLM supply chains are susceptible to various vulnerabilities, which can affect the integrity of training data, models, and deployment platforms. These risks can result in biased outputs, security breaches, or system failures. While traditional software vulnerabilities focus on issues like code flaws and dependencies, in ML the risks also extend to third-party pre-trained models and data. [zdroj](https://genai.owasp.org/llmrisk/llm032025-supply-chain/)
-
- --zaradenie
- Lifecycle:      `artifacts_supply` / `training` / `data_pipeline`
- Harm:            `integrity` / `privacy`
- LLM tag:        [LLM03: 2025 Supply Chain Vulnerabilities ](https://genai.owasp.org/llmrisk/llm032025-supply-chain/)
-
- -- ako utok prebieha
	-
- This opacity makes them susceptible to malicious manipulation, such as backdoors introduced during training or fine-tuning phases, model conversion.These backdoors typically remain dormant under normal conditions, only activating when specific triggers are introduced, leading to behaviors such as data leakage, misclassification, or unauthorized actions. Traditional security measures, including Software Bills of Materials (SBOMs) and static code analysis tools, are ill-suited to detect such threats within AI models.[zdroj](https://www.trendmicro.com/vinfo/nl/security/news/cybercrime-and-digital-threats/exploiting-trust-in-open-source-ai-the-hidden-supply-chain-risk-no-one-is-watching)
-
- vkladanie backdoor alebo skodlive spravanie:
	- pretrained models from opensource repositories (Hugging Face, Github)
	- datasets with triggers (clean-label backdoor)
	- [Python Package Index (PyPI)](https://blog.orsinium.dev/posts/py/pypi-squatting/) and [Node Package Manager (npm)](https://blog.sonatype.com/pypi-and-npm-flooded-with-over-5000-dependency-confusion-copycats) (typosquatting, dependency confusion)
	- pluging and AI agents with own code [zdroj](https://protectai.com/threat-research/unveiling-ai-supply-chain-attacks-on-hugging-face)
-
- Risks:
	- #### [Traditional Third-party Package Vulnerabilities](https://github.com/OWASP/www-project-top-10-for-large-language-model-applications/blob/main/2_0_vulns/LLM03_SupplyChain.md#1-traditional-third-party-package-vulnerabilities)
	- ####   [Vulnerable Pre-Trained Model](https://github.com/OWASP/www-project-top-10-for-large-language-model-applications/blob/main/2_0_vulns/LLM03_SupplyChain.md#4-vulnerable-pre-trained-model)
	- #### [Vulnerable LoRA adapters](https://github.com/OWASP/www-project-top-10-for-large-language-model-applications/blob/main/2_0_vulns/LLM03_SupplyChain.md#6-vulnerable-lora-adapters)
	- #### [Unclear T&Cs and Data Privacy Policies](https://github.com/OWASP/www-project-top-10-for-large-language-model-applications/blob/main/2_0_vulns/LLM03_SupplyChain.md#9-unclear-tcs-and-data-privacy-policies)
	- [zdroj](https://genai.owasp.org/llmrisk/llm032025-supply-chain/)
-
- --mitigation strategies
- First, companies need to treat AI integrations the same way they treat any other third-party dependency—with skepticism until proven otherwise. That means doing full vetting before bringing in an external AI model, tool, or service. Vet the model’s origin, scrutinize the data sources it was trained on, and if possible, run security testing against it before deploying it in production environments.
- Second, organizations should harden their development and deployment pipelines. That includes using Software Bills of Materials (SBOMs). Knowing exactly what models, datasets, and libraries are embedded in your systems—and tracking changes over time—makes it harder for malicious updates or poisoned models to slip through unnoticed.
- Third, API integrations need to be treated with real caution. External AI services shouldn’t be given unchecked access deep into critical systems. Implement strict access controls, monitor API traffic 
  for anomalies, and be ready to pull the plug if an external service starts behaving strangely.
- Fourth, organizations need to extend threat detection and monitoring to include AI behaviors. That means setting up systems that can spot when an AI model suddenly starts behaving in ways that weren’t part of its intended function. Unexpected outputs, unusual data requests, strange 
  error patterns—these could be early warning signs of tampering or compromise. [zdroj](https://www.offsec.com/blog/ai-and-supply-chain-attacks/)
-
- -- priklady/scenarios
- https://owasp.org/www-project-machine-learning-security-top-10/docs/ML06_2023-AI_Supply_Chain_Attacks
-